import collections

import ray
import numpy as np

from typing import Dict, Sequence

from open_spiel.python.policy import (
    Policy as OSPolicy,
    TabularPolicy,
)
from open_spiel.python.algorithms.exploitability import nash_conv
import pyspiel

from malib.utils.typing import PolicyID, AgentID
from malib.envs.tabular.state import State as TabularGameState
from malib.envs.tabular.game import Game as TabularGame


def _state_values(state: TabularGameState, num_players, policy):
    """Value of a state for every player given a policy."""
    if state.is_terminal():
        return np.array(state.returns())
    else:
        p_action = (
            state.chance_outcomes()
            if state.is_chance_node()
            else policy.action_probabilities(state).items()
        )
        return sum(
            prob * _state_values(state.child(action), num_players, policy)
            for action, prob in p_action
        )


def best_response(game, policy, player_id):
    """Returns information about the specified player's best response.

    Given a game and a policy for every player, computes for a single player their
    best unilateral strategy. Returns the value improvement that player would
    get, the action they should take in each information state, and the value
    of each state when following their unilateral policy.

    Args:
      game: An open_spiel game, e.g. kuhn_poker
      policy: A `policy.Policy` object. This policy should depend only on the
        information state available to the current player, but this is not
        enforced.
      player_id: The integer id of a player in the game for whom the best response
        will be computed.

    Returns:
      A dictionary of values, with keys:
        best_response_action: The best unilateral strategy for `player_id` as a
          map from infostatekey to action_id.
        best_response_state_value: The value obtained for `player_id` when
          unilaterally switching strategy, for each state.
        best_response_value: The value obtained for `player_id` when unilaterally
          switching strategy.
        info_sets: A dict of info sets, mapping info state key to a list of
          `(state, counterfactual_reach_prob)` pairs.
        nash_conv: `best_response_value - on_policy_value`
        on_policy_value: The value for `player_id` when all players follow the
          policy
        on_policy_values: The value for each player when all players follow the
          policy
    """
    root_state = game.new_initial_state()
    br = BestResponsePolicy(game, player_id, policy, root_state)
    on_policy_values = _state_values(root_state, game.num_players(), policy)
    best_response_value = br.value(root_state)

    # Get best response action for unvisited states
    for infostate in set(br.infosets) - set(br.cache_best_response_action):
        br.best_response_action(infostate)

    return {
        "best_response_action": br.cache_best_response_action,
        "best_response_state_value": br.cache_value,
        "best_response_value": best_response_value,
        "info_sets": br.infosets,
        "nash_conv": best_response_value - on_policy_values[player_id],
        "on_policy_value": on_policy_values[player_id],
        "on_policy_values": on_policy_values,
    }


_NashConvReturn = collections.namedtuple(
    "_NashConvReturn", ["nash_conv", "player_improvements"]
)


def nash_conv_v2(
    game: TabularGame, policies, return_only_nash_conv=True, use_cpp_br=False
):
    r"""Returns a measure of closeness to Nash for a policy in the game.

    See https://arxiv.org/pdf/1711.00832.pdf for the NashConv definition.

    Args:
      game: An open_spiel game, e.g. kuhn_poker
      policy: A `policy.Policy` object. This policy should depend only on the
        information state available to the current player, but this is not
        enforced.
      return_only_nash_conv: Whether to only return the NashConv value, or a
        namedtuple containing additional statistics. Prefer using `False`, as we
        hope to change the default to that value.
      use_cpp_br: if True, compute the best response in c++

    Returns:
      Returns a object with the following attributes:
      - player_improvements: A `[num_players]` numpy array of the improvement
        for players (i.e. value_player_p_versus_BR - value_player_p).
      - nash_conv: The sum over all players of the improvements in value that each
        player could obtain by unilaterally changing their strategy, i.e.
        sum(player_improvements).
    """
    root_state = game.initial_states()
    best_response_values = np.array(
        [
            BestResponsePolicy(game, best_responder, policies).value(root_state)
            for best_responder in range(game.game_spec.num_players)
        ]
    )
    on_policy_values = _state_values(root_state, game.game_spec.num_players, policies)
    player_improvements = best_response_values - on_policy_values
    nash_conv_ = sum(player_improvements)
    if return_only_nash_conv:
        return nash_conv_
    else:
        return _NashConvReturn(
            nash_conv=nash_conv_, player_improvements=player_improvements
        )


def measure_exploitabilty_v2(
    game_name: str,
    agent_interfaces: Dict[AgentID, "AgentInterface"],
    policy_mixture_dict: Dict[AgentID, Dict[PolicyID, float]],
):
    # TODO(ming): create game
    game = TabularGame()
    # convert policy pool to a policy mixture
    policies = {}
    for interface in agent_interfaces.values():
        env_agents = ray.get(interface.agent_group.remote())
        policies.update(
            {
                aid: ray.get(
                    interface.policy_pool_mixture.remote(
                        policy_mixture_dict[aid], tabular=True
                    )
                )
                for aid in env_agents
            }
        )

    for env_aid, policy in policies.items():
        # TODO(ming): implementation is required
        policy.set_states_to_init_policy(game.states.filter(player=env_aid))

    # TODO(ming): replace the implementation provided by open-spiel
    return nash_conv_v2(game=game, policies=policies, return_only_nash_conv=False)
