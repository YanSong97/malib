import collections

import ray
import numpy as np

from malib.utils.typing import PolicyID, AgentID, Dict, Any
from malib.algorithm.common.policy import TabularPolicy
from malib.envs.tabular.state import State as TabularGameState
from malib.envs.tabular.game import Game as TabularGame, _memory_cache


def _online_state_values(
    state: TabularGameState, policies: Dict[AgentID, TabularPolicy]
):
    """Compute the online discounted state value by rollout"""

    if state.is_terminal():
        return 0.0
    else:
        p_action = (
            state.chance_outcomes()
            if state.is_chance_node()
            else policies[state.current_player()].action_probability(state).items()
        )
        value = 0.0
        for action, prob in p_action:
            value += prob * (
                state.reward(action)
                + state.discounted * _online_state_values(state.next(action), policies)
            )
        return value


_NashConvReturn = collections.namedtuple(
    "_NashConvReturn", "nash_conv, player_improvements"
)


class BRWalker:
    def __init__(
        self,
        game: TabularGame,
        best_responder: int,
        br: TabularPolicy,
        policies: Dict[AgentID, TabularPolicy],
    ):

        # walk through until end
        executors = {aid: policy for aid, policy in policies.items()}
        executors[best_responder] = br

        self._infosets = game.info_sets(best_responder, executors)
        self._player = best_responder
        self._br = br
        self._policies = policies
        self._game = game

    @_memory_cache()
    def value(self, state: TabularGameState) -> float:
        if state.is_terminal():
            return state.player_return(self._player)
        elif state.current_player() == self._player:
            infoset = self._infosets[state.information_state_string(self._player)]
            action = self.best_response_action(infoset)
            return state.reward(action) + self.value(state.next(action))
        else:
            # we do not apply reward to the ego player
            return sum(
                p * self.value(state.next(a)) for a, p in self._game.transition(state)
            )

    def best_response_action(self, infoset):
        head_node = infoset[0]
        head_state: TabularGameState = head_node[0]
        return max(
            head_state.legal_actions_mask(),
            key=lambda a: sum(
                cum_prob * self.value(state) for state, cum_prob in infoset
            ),
        )


def nash_conv_v2(
    game: TabularGame,
    brs: Dict[AgentID, TabularPolicy],
    policies: Dict[AgentID, TabularPolicy],
    return_only_nash_conv=True,
) -> _NashConvReturn:
    """Returns a measure of closeness to Nash for a policy in the game.

    See https://arxiv.org/pdf/1711.00832.pdf for the NashConv definition.
    """
    root_state = game.initial_states()
    # walk through the game tree to get state values
    best_response_values = {
        best_responder: BRWalker(game, best_responder, br, policies).value(root_state)
        for best_responder, br in brs.items()
    }
    on_policy_values = _online_state_values(root_state, policies)
    player_improvements = {
        player: best_response_values[player] - on_policy_values[player]
        for player in brs
    }
    nash_conv_ = sum(player_improvements.values())

    if return_only_nash_conv:
        return _NashConvReturn(nash_conv=nash_conv_, player_improvements=None)
    else:
        return _NashConvReturn(
            nash_conv=nash_conv_, player_improvements=player_improvements
        )


def measure_exploitabilty_v2(
    env_desc: Dict[str, Any],
    agent_interfaces: Dict[AgentID, "AgentInterface"],
    brs: Dict[AgentID, PolicyID],
    policy_mixture_dict: Dict[AgentID, Dict[PolicyID, float]],
) -> _NashConvReturn:
    # TODO(ming): create game
    env = env_desc["creator"](**env_desc["config"])
    assert (
        env.game_spec is not None
    ), "Environment instance has no game spec is registered, cannot be converted to a tabular game!"
    game = TabularGame.from_game_spec(env.game_spec)
    env.close()
    del env

    # convert policy pool to a policy mixture
    policies = {}
    _best_responses = {}
    for interface in agent_interfaces.values():
        # env_agents = ray.get(interface.agent_group.remote())
        policies.update(
            ray.get(
                interface.get_policy_pool_mixture.remote(
                    policy_mixture_dict, tabular=True
                )
            )
        )
        _best_responses.update(
            ray.get(interface.get_policies_with_mapping.remote(brs, tabular=True))
        )

    for env_aid, policy in policies.items():
        # TODO(ming): implementation is required
        policy.set_states_to_init_policy(game.states.filter(player=env_aid))

    return nash_conv_v2(
        game=game, brs=_best_responses, policies=policies, return_only_nash_conv=False
    )
