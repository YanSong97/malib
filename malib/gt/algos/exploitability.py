import collections
from typing_extensions import Annotated

import ray
import numpy as np

from malib.utils.typing import PolicyID, AgentID, Dict, Any
from malib.algorithm.common.policy import TabularPolicy
from malib.envs.tabular.state import State as TabularGameState
from malib.envs.tabular.game import Game as TabularGame


def _online_state_values(
    state: TabularGameState, policies: Dict[AgentID, TabularPolicy]
):
    """Compute the online discounted state value by rollout"""

    if state.is_terminal():
        return 0.0
    else:
        p_action = (
            state.chance_outcomes()
            if state.is_chance_node()
            else policies[state.current_player()].action_probability(state).items()
        )
        value = 0.0
        for action, prob in p_action:
            value += prob * (
                state.reward(action)
                + state.discounted * _online_state_values(state.next(action), policies)
            )
        return value


def best_response(game: TabularGame, policies: TabularGame, player_id: AgentID):
    """Returns information about the specified player's best response.

    Given a game and a policy for every player, computes for a single player their
    best unilateral strategy. Returns the value improvement that player would
    get, the action they should take in each information state, and the value
    of each state when following their unilateral policy.

    Args:
      game: An open_spiel game, e.g. kuhn_poker
      policy: A `policy.Policy` object. This policy should depend only on the
        information state available to the current player, but this is not
        enforced.
      player_id: The integer id of a player in the game for whom the best response
        will be computed.

    Returns:
      A dictionary of values, with keys:
        best_response_action: The best unilateral strategy for `player_id` as a
          map from infostatekey to action_id.
        best_response_state_value: The value obtained for `player_id` when
          unilaterally switching strategy, for each state.
        best_response_value: The value obtained for `player_id` when unilaterally
          switching strategy.
        info_sets: A dict of info sets, mapping info state key to a list of
          `(state, counterfactual_reach_prob)` pairs.
        nash_conv: `best_response_value - on_policy_value`
        on_policy_value: The value for `player_id` when all players follow the
          policy
        on_policy_values: The value for each player when all players follow the
          policy
    """

    root_state = game.initial_states()
    br = BestResponsePolicy(game, player_id, policy, root_state)
    on_policy_values = _online_state_values(root_state, policies)
    best_response_value = br.value(root_state)

    # Get best response action for unvisited states
    for infostate in set(br.infosets) - set(br.cache_best_response_action):
        br.best_response_action(infostate)

    return {
        "best_response_action": br.cache_best_response_action,
        "best_response_state_value": br.cache_value,
        "best_response_value": best_response_value,
        "info_sets": br.infosets,
        "nash_conv": best_response_value - on_policy_values[player_id],
        "on_policy_value": on_policy_values[player_id],
        "on_policy_values": on_policy_values,
    }


_NashConvReturn = collections.namedtuple(
    "_NashConvReturn", "nash_conv, player_improvements"
)


class BRWalker:
    def __init__(
        self,
        game: TabularGame,
        best_responder: int,
        brs: Dict[AgentID, TabularPolicy],
        policies: Dict[AgentID, TabularPolicy],
    ):
        cur_state = game.initial_state()

        # walk through until end
        executors = {aid: policy for aid, policy in policies.items()}
        executors[best_responder] = brs[best_responder]
        action = executors[cur_state.current_player](cur_state)
        # next_state = game.step(action)
        last_state = None
        for player, cur_state in game.agent_iter():
            # observation, reward, done, info = game._env.last()
            cur_state.add_transition(action, next_state)
            cur_state = next_state
            action = executors[cur_state.current_player](cur_state)
            game.step(action)


def nash_conv_v2(
    game: TabularGame,
    brs: Dict[AgentID, TabularPolicy],
    policies: Dict[AgentID, TabularPolicy],
    return_only_nash_conv=True,
) -> _NashConvReturn:
    r"""Returns a measure of closeness to Nash for a policy in the game.

    See https://arxiv.org/pdf/1711.00832.pdf for the NashConv definition.
    """
    root_state = game.initial_states()
    # walk through the game tree to get state values
    best_response_values = np.array(
        [
            BRWalker(game, best_responder, brs, policies).value(root_state)
            for best_responder in range(game.game_spec.num_players)
        ]
    )
    on_policy_values = _online_state_values(root_state, policies)
    player_improvements = best_response_values - on_policy_values
    nash_conv_ = sum(player_improvements)
    if return_only_nash_conv:
        return _NashConvReturn(nash_conv=nash_conv_, player_improvements=None)
    else:
        return _NashConvReturn(
            nash_conv=nash_conv_, player_improvements=player_improvements
        )


def measure_exploitabilty_v2(
    env_desc: Dict[str, Any],
    agent_interfaces: Dict[AgentID, "AgentInterface"],
    policy_mixture_dict: Dict[AgentID, Dict[PolicyID, float]],
) -> _NashConvReturn:
    # TODO(ming): create game
    env = env_desc["creator"](**env_desc["config"])
    assert (
        env.game_spec is not None
    ), "Environment instance has no game spec is registered, cannot be converted to a tabular game!"
    game = TabularGame.from_game_spec(env.game_spec)
    env.close()
    del env

    # convert policy pool to a policy mixture
    policies = {}
    for interface in agent_interfaces.values():
        env_agents = ray.get(interface.agent_group.remote())
        policies.update(
            {
                aid: ray.get(
                    interface.policy_pool_mixture.remote(
                        policy_mixture_dict[aid], tabular=True
                    )
                )
                for aid in env_agents
            }
        )

    for env_aid, policy in policies.items():
        # TODO(ming): implementation is required
        policy.set_states_to_init_policy(game.states.filter(player=env_aid))

    # TODO(ming): replace the implementation provided by open-spiel
    return nash_conv_v2(game=game, policies=policies, return_only_nash_conv=False)
