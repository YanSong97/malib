import collections

import ray
import numpy as np

from malib.utils.typing import PolicyID, AgentID, Dict, Any
from malib.algorithm.common.policy import TabularPolicy
from malib.envs.tabular.state import State as TabularGameState
from malib.envs.tabular.game import Game as TabularGame


def _online_state_values(
    state: TabularGameState, policies: Dict[AgentID, TabularPolicy]
):
    """Compute the online discounted state value by rollout"""

    if state.is_terminal():
        return 0.0
    else:
        p_action = (
            state.chance_outcomes()
            if state.is_chance_node()
            else policies[state.current_player()].action_probability(state).items()
        )
        value = 0.0
        for action, prob in p_action:
            value += prob * (
                state.reward(action)
                + state.discounted * _online_state_values(state.next(action), policies)
            )
        return value


# def best_response(game: TabularGame, policies: TabularGame, player_id: AgentID):
#     """Returns information about the specified player's best response.

#     Given a game and a policy for every player, computes for a single player their
#     best unilateral strategy. Returns the value improvement that player would
#     get, the action they should take in each information state, and the value
#     of each state when following their unilateral policy.

#     Args:
#       game: An open_spiel game, e.g. kuhn_poker
#       policy: A `policy.Policy` object. This policy should depend only on the
#         information state available to the current player, but this is not
#         enforced.
#       player_id: The integer id of a player in the game for whom the best response
#         will be computed.

#     Returns:
#       A dictionary of values, with keys:
#         best_response_action: The best unilateral strategy for `player_id` as a
#           map from infostatekey to action_id.
#         best_response_state_value: The value obtained for `player_id` when
#           unilaterally switching strategy, for each state.
#         best_response_value: The value obtained for `player_id` when unilaterally
#           switching strategy.
#         info_sets: A dict of info sets, mapping info state key to a list of
#           `(state, counterfactual_reach_prob)` pairs.
#         nash_conv: `best_response_value - on_policy_value`
#         on_policy_value: The value for `player_id` when all players follow the
#           policy
#         on_policy_values: The value for each player when all players follow the
#           policy
#     """

#     root_state = game.initial_states()
#     br = BestResponsePolicy(game, player_id, policy, root_state)
#     on_policy_values = _online_state_values(root_state, policies)
#     best_response_value = br.value(root_state)

#     # Get best response action for unvisited states
#     for infostate in set(br.infosets) - set(br.cache_best_response_action):
#         br.best_response_action(infostate)

#     return {
#         "best_response_action": br.cache_best_response_action,
#         "best_response_state_value": br.cache_value,
#         "best_response_value": best_response_value,
#         "info_sets": br.infosets,
#         "nash_conv": best_response_value - on_policy_values[player_id],
#         "on_policy_value": on_policy_values[player_id],
#         "on_policy_values": on_policy_values,
#     }


_NashConvReturn = collections.namedtuple(
    "_NashConvReturn", "nash_conv, player_improvements"
)


class BRWalker:
    def __init__(
        self,
        game: TabularGame,
        best_responder: int,
        br: TabularPolicy,
        policies: Dict[AgentID, TabularPolicy],
    ):

        # walk through until end
        executors = {aid: policy for aid, policy in policies.items()}
        executors[best_responder] = br

        self._infosets = game.info_sets(best_responder, executors)
        self._player = best_responder
        self._br = br
        self._policies = policies
        self._game = game

    def value(self, state: TabularGameState) -> float:
        # calculate state with ...
        if state.is_terminal():
            return state.player_return(self._player)
        elif state.current_player() == self._player:
            infoset = self._infosets[state.information_state_string(self._player)]
            action = self.best_response_action(infoset)
            return state.reward(action) + self.value(state.next(action))
        else:
            return sum(
                p * (state.reward(a) + self.value(state))
                for a, p in self._game.transition(state)
            )
        # values = _online_state_values(state, self.executors)
        # raise values[self.best_responder]

    def best_response_action(self, infoset):
        head_node = infoset[0]
        head_state: TabularGameState = head_node[0]
        return max(
            head_state.legal_actions_mask(),
            key=lambda a: sum(
                cum_prob * self.value(state) for state, cum_prob in infoset
            ),
        )


def nash_conv_v2(
    game: TabularGame,
    brs: Dict[AgentID, TabularPolicy],
    policies: Dict[AgentID, TabularPolicy],
    return_only_nash_conv=True,
) -> _NashConvReturn:
    """Returns a measure of closeness to Nash for a policy in the game.

    See https://arxiv.org/pdf/1711.00832.pdf for the NashConv definition.
    """
    root_state = game.initial_states()
    # walk through the game tree to get state values
    best_response_values = {
        best_responder: BRWalker(game, best_responder, br, policies).value(root_state)
        for best_responder, br in brs.items()
    }
    on_policy_values = _online_state_values(root_state, policies)
    player_improvements = {
        player: best_response_values[player] - on_policy_values[player]
        for player in brs
    }
    nash_conv_ = sum(player_improvements.values())

    if return_only_nash_conv:
        return _NashConvReturn(nash_conv=nash_conv_, player_improvements=None)
    else:
        return _NashConvReturn(
            nash_conv=nash_conv_, player_improvements=player_improvements
        )


def measure_exploitabilty_v2(
    env_desc: Dict[str, Any],
    agent_interfaces: Dict[AgentID, "AgentInterface"],
    brs: Dict[AgentID, PolicyID],
    policy_mixture_dict: Dict[AgentID, Dict[PolicyID, float]],
) -> _NashConvReturn:
    # TODO(ming): create game
    env = env_desc["creator"](**env_desc["config"])
    assert (
        env.game_spec is not None
    ), "Environment instance has no game spec is registered, cannot be converted to a tabular game!"
    game = TabularGame.from_game_spec(env.game_spec)
    env.close()
    del env

    # convert policy pool to a policy mixture
    policies = {}
    _best_responses = {}
    for interface in agent_interfaces.values():
        # env_agents = ray.get(interface.agent_group.remote())
        policies.update(
            ray.get(
                interface.get_policy_pool_mixture.remote(
                    policy_mixture_dict, tabular=True
                )
            )
        )
        _best_responses.update(
            ray.get(interface.get_policies_with_mapping.remote(brs, tabular=True))
        )

    for env_aid, policy in policies.items():
        # TODO(ming): implementation is required
        policy.set_states_to_init_policy(game.states.filter(player=env_aid))

    return nash_conv_v2(
        game=game, brs=_best_responses, policies=policies, return_only_nash_conv=False
    )
