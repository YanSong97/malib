import collections

import ray
import numpy as np

from malib.utils.typing import PolicyID, AgentID, Dict, Any
from malib.algorithm.common.policy import TabularPolicy
from malib.envs.tabular.state import State as TabularGameState
from malib.envs.tabular.game import Game as TabularGame, _memory_cache


def _online_state_values(
    state: TabularGameState, policies: Dict[AgentID, TabularPolicy]
) -> Dict[AgentID, float]:
    """Compute the online discounted state value by rollout"""

    if state.is_terminal():
        return {player: 0.0 for player in policies}
    else:
        p_action = (
            state.chance_outcomes()
            if state.is_chance_node()
            else policies[state.current_player()].action_probability(state).items()
        )
        value = {player: 0.0 for player in policies}
        for action, prob in p_action:
            value[state.current_player()] += prob * (
                state.reward(action)
                + state.discounted
                * _online_state_values(state.next(action), policies)[
                    state.current_player()
                ]
            )
        return value


_NashConvReturn = collections.namedtuple(
    "_NashConvReturn", "nash_conv, player_improvements"
)


class BRWalker:
    def __init__(
        self,
        game: TabularGame,
        best_responder: int,
        br: TabularPolicy,
        policies: Dict[AgentID, TabularPolicy],
    ):

        # walk through until end
        executors = {aid: policy for aid, policy in policies.items()}
        executors[best_responder] = br

        self._infosets = game.info_sets(best_responder, executors)
        self._player = best_responder
        self._br = br
        self._policies = policies
        self._game = game

    @_memory_cache(lambda *arg: "_".join(map(str, arg)))
    def value(self, state: TabularGameState) -> float:
        if state.is_terminal():
            return state.player_return(self._player)
        elif state.current_player() == self._player:
            infoset = self._infosets[state.information_state_string(self._player)]
            action = self.best_response_action(infoset)
            return state.reward(action) + state.discounted * self.value(
                state.next(action)
            )
        else:
            # we do not apply reward to the ego player
            return sum(
                p * self.value(state.next(a)) for a, p in self._game.transition(state)
            )

    def best_response_action(self, infoset):
        # infoset: [(state, probability), ...]
        head_node = infoset[0]
        head_state: TabularGameState = head_node[0]
        return max(
            head_state.legal_actions_mask(),
            key=lambda a: sum(
                cum_prob * self.value(state) for state, cum_prob in infoset
            ),
        )


def nash_conv_v2(
    env_desc: Dict[str, Any],
    brs: Dict[AgentID, TabularPolicy],
    policies: Dict[AgentID, TabularPolicy],
) -> _NashConvReturn:
    """Returns a measure of closeness to Nash for a policy in the game.

    See https://arxiv.org/pdf/1711.00832.pdf for the NashConv definition.
    """

    game = TabularGame(env_desc)
    root_state = game.initial_state()
    # walk through the game tree to get state values
    best_response_values = {
        best_responder: BRWalker(game, best_responder, br, policies).value(root_state)
        for best_responder, br in brs.items()
    }
    on_policy_values = _online_state_values(root_state, policies)
    player_improvements = {
        player: best_response_values[player] - on_policy_values[player]
        for player in brs
    }
    nash_conv_ = sum(player_improvements.values())

    return _NashConvReturn(
        nash_conv=nash_conv_, player_improvements=player_improvements
    )


# def measure_exploitabilty_v2(
#     env_desc: Dict[str, Any],
#     agent_interfaces: Dict[AgentID, "AgentInterface"],
#     brs: Dict[AgentID, PolicyID],
#     policy_mixture_dict: Dict[AgentID, Dict[PolicyID, float]],
# ) -> _NashConvReturn:

#     game = TabularGame(env_desc)
#     # convert policy pool to a policy mixture
#     policies = {}
#     _best_responses = {}
#     for interface in agent_interfaces.values():
#         # env_agents = ray.get(interface.agent_group.remote())
#         policies.update(
#             ray.get(
#                 interface.get_policy_pool_mixture.remote(
#                     policy_mixture_dict, tabular=True
#                 )
#             )
#         )
#         _best_responses.update(
#             ray.get(interface.get_policies_with_mapping.remote(brs, tabular=True))
#         )

#     return nash_conv_v2(
#         game=game, brs=_best_responses, policies=policies, return_only_nash_conv=False
#     )
