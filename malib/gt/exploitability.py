import collections
import logging

import numpy as np

from malib.utils.typing import PolicyID, AgentID, Dict, Any
from malib.algorithm.common.policy import TabularPolicy
from malib.envs.tabular.state import State as TabularGameState
from malib.envs.tabular.game import Game as TabularGame, _memory_cache


def _online_state_values(
    state: TabularGameState, policies: Dict[AgentID, TabularPolicy]
) -> Dict[AgentID, float]:
    """Compute the online discounted state value by rollout"""
    # FIXME(ming): handling unexplored nodes
    if state is None:
        return {player: 0.0 for player in policies}

    if state.is_terminal():
        return state.player_returns
    else:
        p_action = (
            state.chance_outcomes()
            if state.is_chance_node()
            else policies[state.current_player()].action_probability(state).items()
        )
        value = {player: 0.0 for player in policies}
        for action, prob in p_action:
            value[state.current_player()] += prob * (
                state.reward(action)
                + state.discounted
                * _online_state_values(state.next(action), policies)[
                    state.current_player()
                ]
            )
        return value


_NashConvReturn = collections.namedtuple(
    "_NashConvReturn", "nash_conv, player_improvements"
)


class BRWalker:
    def __init__(
        self,
        game: TabularGame,
        best_responder: int,
        br: TabularPolicy,
        policies: Dict[AgentID, TabularPolicy],
        seed=None,
    ):

        # walk through until end
        executors = {aid: policy for aid, policy in policies.items()}
        executors[best_responder] = br

        self._infosets = game.infosets(best_responder, executors, seed)
        # logging.debug(f"{best_responder}: {sorted(list(self._infosets.keys()))}")
        self._player = best_responder
        self._br = br
        self._policies = policies
        self._game = game
        self._executors = executors

    # @_memory_cache(lambda *arg: "_".join(map(str, arg)))
    def value(self, state: TabularGameState) -> float:
        # FIXME(ming): handling unexplored nodes
        if state is None:
            return 0.0
        if state.is_terminal():
            # logging.debug(f"state info: {state.current_player()}, {state.level} {state._reward_func} {state.game_over}")
            return state.player_returns[self._player]
        elif state.current_player() == self._player:
            assert state.information_state_string(self._player) in self._infosets, list(
                self._infosets.keys()
            )
            infoset = self._infosets[state.information_state_string(self._player)]
            action = self.best_response_action(infoset)
            return state.reward(action) + state.discounted * self.value(
                state.next(action)
            )
        else:
            # we do not apply reward to the non-ego player
            if state.current_player() == self._player:
                items = [(action, 1.0) for action in state.legal_actions_mask]
            elif state.is_chance_node():
                items = state.chance_outcomes()
            else:
                # TODO(ming): we need to filter actions here
                items = list(
                    self._executors[state.current_player()]
                    .action_probability(state, prob_clip=0.0)
                    .items()
                )
            return sum(p * self.value(state.next(a)) for a, p in items)

    def best_response_action(self, infoset):
        # infoset: [(state, probability), ...]
        head_node = infoset[0]
        head_state: TabularGameState = head_node[0]
        return max(
            head_state.legal_actions_mask,
            key=lambda a: sum(
                cum_prob * self.value(state.next(a)) for state, cum_prob in infoset
            ),
        )


def nash_conv_v2(
    env_desc: Dict[str, Any],
    brs: Dict[AgentID, TabularPolicy],
    policies: Dict[AgentID, TabularPolicy],
    seed: int = 0,
) -> _NashConvReturn:
    """Returns a measure of closeness to Nash for a policy in the game.

    See https://arxiv.org/pdf/1711.00832.pdf for the NashConv definition.
    """

    game = TabularGame(env_desc)
    root_state = game.initial_state(seed)
    # walk through the game tree to get state values
    brwalkers = {
        best_responder: BRWalker(game, best_responder, br, policies, seed)
        for best_responder, br in brs.items()
    }
    best_response_values = {k: v.value(root_state) for k, v in brwalkers.items()}
    on_policy_values = _online_state_values(root_state, policies)
    player_improvements = {
        player: best_response_values[player] - on_policy_values[player]
        for player in brs
    }
    nash_conv_ = sum(player_improvements.values())

    return _NashConvReturn(
        nash_conv=nash_conv_, player_improvements=player_improvements
    )


# def measure_exploitabilty_v2(
#     env_desc: Dict[str, Any],
#     agent_interfaces: Dict[AgentID, "AgentInterface"],
#     brs: Dict[AgentID, PolicyID],
#     policy_mixture_dict: Dict[AgentID, Dict[PolicyID, float]],
# ) -> _NashConvReturn:

#     game = TabularGame(env_desc)
#     # convert policy pool to a policy mixture
#     policies = {}
#     _best_responses = {}
#     for interface in agent_interfaces.values():
#         # env_agents = ray.get(interface.agent_group.remote())
#         policies.update(
#             ray.get(
#                 interface.get_policy_pool_mixture.remote(
#                     policy_mixture_dict, tabular=True
#                 )
#             )
#         )
#         _best_responses.update(
#             ray.get(interface.get_policies_with_mapping.remote(brs, tabular=True))
#         )

#     return nash_conv_v2(
#         game=game, brs=_best_responses, policies=policies, return_only_nash_conv=False
#     )
