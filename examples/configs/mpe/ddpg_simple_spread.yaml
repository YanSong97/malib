# battle: https://github.com/wsjeon/maddpg-rllib
group: "MPE"
name: "share/ddpg_simple_spread"
task_mode: marl

training:
  interface:
    type: "independent"
    population_size: -1
    use_init_population_pool: False
  config:
    # control the frequency of remote parameter update
    use_cuda: True
    update_interval: 1
    saving_interval: 10
    batch_size: 1024
    optimizer: "Adam"
    actor_lr: 0.01
    critic_lr: 0.01
    lr: 0.01
    tau: 0.01  # soft update
    grad_norm_clipping: 0.5

rollout_worker:
  callback: simultaneous
  stopper:
    name: simple_rollout
    config:
      max_step: 1000
  num_threads: 2
  num_env_per_thread: 1
  num_eval_threads: 1
  batch_mode: time_step
  post_processor_types:
    - default
  use_subproc_env: False
  task_config:
    max_step: 25
    fragment_length: 500

evaluation:
  fragment_length: 25
  num_episodes: 100

env_description:
  #  scenario_name: "simple_spread"
  creator: "MPE"
  config:
    env_id: "simple_spread_v2"
    scenario_configs:
      max_cycles: 25

algorithms:
  DDPG:
    name: "DDPG"
    model_config:
      actor:
        network: mlp
        layers:
          - units: 64
            activation: ReLU
          - units: 64
            activation: ReLU
        output:
          activation: False
      critic:
        network: mlp
        layers:
          - units: 64
            activation: ReLU
          - units: 64
            activation: ReLU
        output:
          activation: False

    # set hyper parameter
    custom_config:
      gamma: 0.95

global_evaluator:
  name: "generic"

dataset_config:
  episode_capacity: 1000000
  learning_start: 2560
